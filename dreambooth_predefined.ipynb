{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d758034",
   "metadata": {},
   "source": [
    "# Ingmar05/Dreambooth-Stable-Diffusion\n",
    "https://github.com/Ingmar05/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aef18c2",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandatory Settings\n",
    "project_name = \"order1\"\n",
    "\n",
    "#Optional Settings\n",
    "dataset=\"person_ddim\" #@param [\"man_euler\", \"man_unsplash\", \"person_ddim\", \"woman_ddim\", \"artstyle\"]\n",
    "token = \"firstNameLastName\" #This is the unique token you are incorporating into the stable diffusion model.\n",
    "class_word = \"person\" # Match class_word to the category of the regularization images you chose above. Typical uses are \"man\", \"person\", \"woman\"\n",
    "max_training_steps = 2000 # MAX STEPS # How many steps do you want to train for?\n",
    "\n",
    "print(\"✅ Settings Applied\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db118dd8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUILD ENV\n",
    "!pip install numpy==1.23.1\n",
    "!pip install pytorch-lightning==1.7.6\n",
    "!pip install csv-logger\n",
    "!pip install torchmetrics==0.11.1\n",
    "!pip install torch-fidelity==0.3.0\n",
    "!pip install albumentations==1.1.0\n",
    "!pip install opencv-python==4.7.0.72\n",
    "!pip install pudb==2019.2\n",
    "!pip install omegaconf==2.1.1\n",
    "!pip install pillow==9.4.0\n",
    "!pip install einops==0.4.1\n",
    "!pip install transformers==4.25.1\n",
    "!pip install kornia==0.6.7\n",
    "!pip install diffusers[training]==0.3.0\n",
    "!pip install captionizer==1.0.1\n",
    "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "!pip install -e .\n",
    "!pip install huggingface_hub\n",
    "!pip install gitpython\n",
    "\n",
    "# Download the 1.5 sd model\n",
    "from IPython.display import clear_output\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"panopstor/EveryDream\",\n",
    " filename=\"sd_v1-5_vae.ckpt\"\n",
    ")\n",
    "\n",
    "# Move the sd_v1-5_vae.ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} model.ckpt\n",
    "clear_output()\n",
    "print(\"✅ model.ckpt successfully downloaded\")\n",
    "\n",
    "#Download Regularization Images\n",
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git\n",
    "\n",
    "!mkdir -p regularization_images/{dataset}\n",
    "!mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* regularization_images/{dataset}\n",
    "\n",
    "# Training\n",
    "\n",
    "# If you are training a person's face, set this to True\n",
    "i_am_training_a_persons_face = False\n",
    "\n",
    "flip_p_arg = 0.0 if i_am_training_a_persons_face else 0.5\n",
    "\n",
    "# 0 Saves the checkpoint when max_training_steps is reached.\n",
    "# 250 saves the checkpoint every 250 steps as well as when max_training_steps is reached.\n",
    "save_every_x_steps = 0\n",
    "\n",
    "reg_data_root = \"/workspace/Dreambooth-Stable-Diffusion/regularization_images/\" + dataset\n",
    "\n",
    "!rm -rf training_images/.ipynb_checkpoints\n",
    "!python \"main.py\" \\\n",
    " --project_name \"{project_name}\" \\\n",
    " --debug False \\\n",
    " --max_training_steps {max_training_steps} \\\n",
    " --token \"{token}\" \\\n",
    " --training_model \"model.ckpt\" \\\n",
    " --training_images \"/workspace/Dreambooth-Stable-Diffusion/training_images\" \\\n",
    " --regularization_images \"{reg_data_root}\" \\\n",
    " --class_word \"{class_word}\" \\\n",
    " --flip_p {flip_p_arg} \\\n",
    " --save_every_x_steps {save_every_x_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d0139",
   "metadata": {},
   "source": [
    "## Wait for Training to Finish and Download Model\n",
    "\n",
    "Download the trained model from */workspace/Dreambooth-Stable-Diffusion/trained_models/*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
